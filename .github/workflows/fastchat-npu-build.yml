name: FastChat NPU Build
run-name: FastChat NPU Build

on:
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'warning'
        type: choice
        options:
          - info
          - warning
          - debug
      print_tags:
        description: 'True to print to STDOUT'
        required: true
        type: boolean
      tags:
        description: 'Test scenario tags'
        required: true
        type: string

jobs:
  check_runner:
    runs-on: ascend-910b
    steps:
      - name: Check Runner Status
        run: echo "Check runner done!"

  check_npu_environment:
    runs-on: ascend-910b
    needs: check_runner
    strategy:
      matrix:
        machine_type: [single-npu, multi-npu]
    container: 
      image: zhangsibo1129/ubuntu-cann-torch21-py39:latest
      volumes:
        - /usr/local/dcmi:/usr/local/dcmi
        - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
        - /usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64
        - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      options: --network host
               --device /dev/davinci_manager
               --device /dev/devmm_svm
               --device /dev/hisi_hdc
               --device /dev/davinci6
               ${{ matrix.machine_type == 'multi-npu' && '--device /dev/davinci7' || ''}}
    steps:
      - name: NPU-SMI
        run: npu-smi info
      - name: Show torch information
        run: |
          python << EOF 
          if __name__ == '__main__':
              import torch
              import torch_npu
              print(f"Device Name: {torch.npu.get_device_name(0)}")
              print(f"Device Count: {torch.npu.device_count()}")
              print(f"Device Available: {torch.npu.is_available()}")
          EOF

  install_fastchat:
    runs-on: ascend-910b
    needs: check_npu_environment
    container: 
      image: zhangsibo1129/ubuntu-cann-torch21-py39:latest
      volumes:
        - /usr/local/dcmi:/usr/local/dcmi
        - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
        - /usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64
        - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
        - /data/disk3/action/projects/:/opt/projects/
        - /data/disk3/action/models/hub/:/opt/big_models/
        - /data/disk3/action/envs/fschat/:/root/data
      options: --network host
               --device /dev/davinci_manager
               --device /dev/devmm_svm
               --device /dev/hisi_hdc
               --device /dev/davinci6
    steps:
        - name: Get FastChat Latest Code
          uses: actions/checkout@v4
          with:
            repository: lm-sys/FastChat
            ssh-key: ${{ secrets.SECRET_SSH_KEY }}
            path: FastChat
            clean: true
        - name: Pull FastChat
          working-directory: /__w/hf-ci-demo/hf-ci-demo/FastChat
          run: |
            git pull
            pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple
            pip install "fschat[model_worker,webui]" -i https://pypi.tuna.tsinghua.edu.cn/simple
            pip install SentencePiece xformers -i https://pypi.tuna.tsinghua.edu.cn/simple
        - name: Import Verification
          working-directory: /__w/hf-ci-demo/hf-ci-demo/FastChat
          run: |
            python << EOF
            if __name__ == '__main__':
                import torch
                from torch.utils.data import Dataset
                import transformers
                from transformers import Trainer
                from transformers.trainer_pt_utils import LabelSmoother
                from transformers import is_torch_npu_available
                if not is_torch_npu_available():
                    raise ImportError('NPU is not available')
                from transformers import pipeline
                from transformers import AutoConfig
                from transformers import AutoTokenizer
                from transformers import AutoModel
                from fastchat.conversation import SeparatorStyle
                from fastchat.model.model_adapter import get_conversation_template
            EOF
        - name: Persistent Python Interpreter
          run: |
            rm -rf /root/data/torch_npu
            mv /root/miniconda/envs/torch_npu /root/data
