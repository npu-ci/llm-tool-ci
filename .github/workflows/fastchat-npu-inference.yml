name: FastChat NPU Inference
run-name: FastChat NPU Inference

on:
  workflow_run:
    workflows: [FastChat NPU Deploy]
    types:
      - completed

jobs:
  run_tests:
    runs-on: ascend-910b
    container:
      image: zhangsibo1129/ubuntu-cann-torch21-py39:latest
      volumes:
        - /usr/local/dcmi:/usr/local/dcmi
        - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
        - /usr/local/Ascend/driver/lib64:/usr/local/Ascend/driver/lib64
        - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
        - /data/disk3/action/models/hub/:/opt/big_models/
        - /data/disk3/action/envs/fschat/torch_npu:/root/miniconda/envs/torch_npu
        - /data/disk3/action/repo:/__w/llm-tool-ci/llm-tool-ci
        - /data/disk3/action/projects/FastChat:/__w/llm-tool-ci/llm-tool-ci/FastChat
      options: --network host
               --device /dev/davinci_manager
               --device /dev/devmm_svm
               --device /dev/hisi_hdc
               --device /dev/davinci6
               --device /dev/davinci7
    steps:
        - name: Copy_Test_Cases
          working-directory: /__w/llm-tool-ci/llm-tool-ci/llm-tool-ci
          run: |
            cp fastchat_test/cases/inference/* /__w/llm-tool-ci/llm-tool-ci/FastChat/tests/

        - name: run_inference_cases
          working-directory: /__w/llm-tool-ci/llm-tool-ci/FastChat
          run: |
            cp tests/*.txt .
            python tests/test_cli.py


