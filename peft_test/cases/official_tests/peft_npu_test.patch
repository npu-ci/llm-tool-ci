diff --git a/tests/test_adaption_prompt.py b/tests/test_adaption_prompt.py
index 92bbd72..c5428e6 100644
--- a/tests/test_adaption_prompt.py
+++ b/tests/test_adaption_prompt.py
@@ -414,7 +414,7 @@ class AdaptionPromptTester(TestCase, PeftCommonTester):
         """Test that AdaptionPrompt works when Llama using a half-precision model."""
         input_ids = torch.LongTensor([[1, 1, 1], [2, 1, 2]]).to(self.torch_device)
         original = LlamaForCausalLM.from_pretrained(
-            "trl-internal-testing/tiny-random-LlamaForCausalLM", torch_dtype=torch.bfloat16
+            "/opt/big_models/tiny-random-LlamaForCausalLM", torch_dtype=torch.bfloat16
         )
         adapted = get_peft_model(
             original, AdaptionPromptConfig(adapter_layers=2, adapter_len=4, task_type="CAUSAL_LM")
diff --git a/tests/test_auto.py b/tests/test_auto.py
deleted file mode 100644
index 24ace99..0000000
--- a/tests/test_auto.py
+++ /dev/null
@@ -1,192 +0,0 @@
-# coding=utf-8
-# Copyright 2023-present the HuggingFace Inc. team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import tempfile
-import unittest
-
-import torch
-
-from peft import (
-    AutoPeftModel,
-    AutoPeftModelForCausalLM,
-    AutoPeftModelForFeatureExtraction,
-    AutoPeftModelForQuestionAnswering,
-    AutoPeftModelForSeq2SeqLM,
-    AutoPeftModelForSequenceClassification,
-    AutoPeftModelForTokenClassification,
-    PeftModel,
-    PeftModelForCausalLM,
-    PeftModelForFeatureExtraction,
-    PeftModelForQuestionAnswering,
-    PeftModelForSeq2SeqLM,
-    PeftModelForSequenceClassification,
-    PeftModelForTokenClassification,
-)
-
-
-class PeftAutoModelTester(unittest.TestCase):
-    def test_peft_causal_lm(self):
-        model_id = "peft-internal-testing/tiny-OPTForCausalLM-lora"
-        model = AutoPeftModelForCausalLM.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForCausalLM))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForCausalLM.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForCausalLM))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForCausalLM))
-        self.assertTrue(model.base_model.lm_head.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForCausalLM.from_pretrained(model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16)
-
-    def test_peft_seq2seq_lm(self):
-        model_id = "peft-internal-testing/tiny_T5ForSeq2SeqLM-lora"
-        model = AutoPeftModelForSeq2SeqLM.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForSeq2SeqLM))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForSeq2SeqLM.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForSeq2SeqLM))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForSeq2SeqLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForSeq2SeqLM))
-        self.assertTrue(model.base_model.lm_head.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForSeq2SeqLM.from_pretrained(model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16)
-
-    def test_peft_sequence_cls(self):
-        model_id = "peft-internal-testing/tiny_OPTForSequenceClassification-lora"
-        model = AutoPeftModelForSequenceClassification.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForSequenceClassification))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForSequenceClassification.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForSequenceClassification))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForSequenceClassification.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForSequenceClassification))
-        self.assertTrue(model.score.original_module.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForSequenceClassification.from_pretrained(
-            model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16
-        )
-
-    def test_peft_token_classification(self):
-        model_id = "peft-internal-testing/tiny_GPT2ForTokenClassification-lora"
-        model = AutoPeftModelForTokenClassification.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForTokenClassification))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForTokenClassification.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForTokenClassification))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForTokenClassification.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForTokenClassification))
-        self.assertTrue(model.base_model.classifier.original_module.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForTokenClassification.from_pretrained(
-            model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16
-        )
-
-    def test_peft_question_answering(self):
-        model_id = "peft-internal-testing/tiny_OPTForQuestionAnswering-lora"
-        model = AutoPeftModelForQuestionAnswering.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForQuestionAnswering))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForQuestionAnswering.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForQuestionAnswering))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForQuestionAnswering.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForQuestionAnswering))
-        self.assertTrue(model.base_model.qa_outputs.original_module.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForQuestionAnswering.from_pretrained(
-            model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16
-        )
-
-    def test_peft_feature_extraction(self):
-        model_id = "peft-internal-testing/tiny_OPTForFeatureExtraction-lora"
-        model = AutoPeftModelForFeatureExtraction.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModelForFeatureExtraction))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModelForFeatureExtraction.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModelForFeatureExtraction))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModelForFeatureExtraction.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModelForFeatureExtraction))
-        self.assertTrue(model.base_model.model.decoder.embed_tokens.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModelForFeatureExtraction.from_pretrained(
-            model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16
-        )
-
-    def test_peft_whisper(self):
-        model_id = "peft-internal-testing/tiny_WhisperForConditionalGeneration-lora"
-        model = AutoPeftModel.from_pretrained(model_id)
-        self.assertTrue(isinstance(model, PeftModel))
-
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            model.save_pretrained(tmp_dirname)
-
-            model = AutoPeftModel.from_pretrained(model_id)
-            self.assertTrue(isinstance(model, PeftModel))
-
-        # check if kwargs are passed correctly
-        model = AutoPeftModel.from_pretrained(model_id, torch_dtype=torch.bfloat16)
-        self.assertTrue(isinstance(model, PeftModel))
-        self.assertTrue(model.base_model.model.model.encoder.embed_positions.weight.dtype == torch.bfloat16)
-
-        adapter_name = "default"
-        is_trainable = False
-        # This should work
-        _ = AutoPeftModel.from_pretrained(model_id, adapter_name, is_trainable, torch_dtype=torch.bfloat16)
diff --git a/tests/test_config.py b/tests/test_config.py
index 7b224bc..d3bc89a 100644
--- a/tests/test_config.py
+++ b/tests/test_config.py
@@ -38,7 +38,7 @@ from peft import (
 )


-PEFT_MODELS_TO_TEST = [("lewtun/tiny-random-OPTForCausalLM-delta", "v1")]
+PEFT_MODELS_TO_TEST = [("/opt/big_models/tiny-random-OPTForCausalLM-delta", "v1")]

 ALL_CONFIG_CLASSES = (
     AdaptionPromptConfig,
@@ -126,14 +126,6 @@ class PeftConfigTester(unittest.TestCase):
                 # Test we can load config from delta
                 config_class.from_pretrained(model_name, revision=revision, cache_dir=tmp_dirname)

-    def test_from_pretrained_cache_dir_remote(self):
-        r"""
-        Test if the config is correctly loaded with a checkpoint from the hub
-        """
-        with tempfile.TemporaryDirectory() as tmp_dirname:
-            PeftConfig.from_pretrained("ybelkada/test-st-lora", cache_dir=tmp_dirname)
-            self.assertTrue("models--ybelkada--test-st-lora" in os.listdir(tmp_dirname))
-
     @parameterized.expand(ALL_CONFIG_CLASSES)
     def test_set_attributes(self, config_class):
         # manually set attributes and check if they are correctly written
diff --git a/tests/test_decoder_models.py b/tests/test_decoder_models.py
index ab49c3e..f57f7b6 100644
--- a/tests/test_decoder_models.py
+++ b/tests/test_decoder_models.py
@@ -25,14 +25,14 @@ from .testing_common import PeftCommonTester, PeftTestConfigManager


 PEFT_DECODER_MODELS_TO_TEST = [
-    "hf-internal-testing/tiny-random-OPTForCausalLM",
-    "hf-internal-testing/tiny-random-GPTNeoXForCausalLM",
-    "hf-internal-testing/tiny-random-GPT2LMHeadModel",
-    "hf-internal-testing/tiny-random-BloomForCausalLM",
-    "hf-internal-testing/tiny-random-gpt_neo",
-    "hf-internal-testing/tiny-random-GPTJForCausalLM",
-    "hf-internal-testing/tiny-random-GPTBigCodeForCausalLM",
-    "HuggingFaceM4/tiny-random-LlamaForCausalLM",
+    "/opt/big_models/tiny-random-OPTForCausalLM",
+    "/opt/big_models/tiny-random-GPTNeoXForCausalLM",
+    "/opt/big_models/tiny-random-GPT2LMHeadModel",
+    "/opt/big_models/tiny-random-BloomForCausalLM",
+    "/opt/big_models/tiny-random-gpt_neo",
+    "/opt/big_models/tiny-random-GPTJForCausalLM",
+    "/opt/big_models/tiny-random-GPTBigCodeForCausalLM",
+    "/opt/big_models/tiny-random-LlamaForCausalLM",
 ]

 FULL_GRID = {
@@ -99,7 +99,7 @@ class PeftDecoderModelTester(unittest.TestCase, PeftCommonTester):
             mock(*args, **kwargs)
             return orig_from_pretrained(config.tokenizer_name_or_path)

-        model_id = "hf-internal-testing/tiny-random-OPTForCausalLM"
+        model_id = "/opt/big_models/tiny-random-OPTForCausalLM"
         config = PromptTuningConfig(
             base_model_name_or_path=model_id,
             tokenizer_name_or_path=model_id,
@@ -119,7 +119,7 @@ class PeftDecoderModelTester(unittest.TestCase, PeftCommonTester):
     def test_prompt_tuning_config_invalid_args(self):
         # Raise an error when tokenizer_kwargs is used with prompt_tuning_init!='TEXT', because this argument has no
         # function in that case
-        model_id = "hf-internal-testing/tiny-random-OPTForCausalLM"
+        model_id = "/opt/big_models/tiny-random-OPTForCausalLM"
         msg = "tokenizer_kwargs only valid when using prompt_tuning_init='TEXT'."
         with self.assertRaisesRegex(ValueError, expected_regex=msg):
             PromptTuningConfig(
@@ -286,7 +286,7 @@ class PeftDecoderModelTester(unittest.TestCase, PeftCommonTester):

     def test_generate_adalora_no_dropout(self):
         # test for issue #730
-        model_id = "hf-internal-testing/tiny-random-OPTForCausalLM"
+        model_id = "/opt/big_models/tiny-random-OPTForCausalLM"
         config_kwargs = {
             "target_modules": None,
             "task_type": "CAUSAL_LM",
diff --git a/tests/test_encoder_decoder_models.py b/tests/test_encoder_decoder_models.py
index 8aab9ed..7356783 100644
--- a/tests/test_encoder_decoder_models.py
+++ b/tests/test_encoder_decoder_models.py
@@ -25,8 +25,8 @@ from .testing_common import PeftCommonTester, PeftTestConfigManager


 PEFT_ENCODER_DECODER_MODELS_TO_TEST = [
-    "ybelkada/tiny-random-T5ForConditionalGeneration-calibrated",
-    "hf-internal-testing/tiny-random-BartForConditionalGeneration",
+    "/opt/big_models/tiny-random-T5ForConditionalGeneration-calibrated",
+    "/opt/big_models/tiny-random-BartForConditionalGeneration",
 ]

 FULL_GRID = {"model_ids": PEFT_ENCODER_DECODER_MODELS_TO_TEST, "task_type": "SEQ_2_SEQ_LM"}
@@ -195,7 +195,7 @@ class PeftEncoderDecoderCustomModelTester(unittest.TestCase):
     """

     def test_save_shared_tensors(self):
-        model_id = "hf-internal-testing/tiny-random-RobertaModel"
+        model_id = "/opt/big_models/tiny-random-RobertaModel"
         peft_config = LoraConfig(
             task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias="all"
         )
diff --git a/tests/test_feature_extraction_models.py b/tests/test_feature_extraction_models.py
index 2b4331a..556f1cd 100644
--- a/tests/test_feature_extraction_models.py
+++ b/tests/test_feature_extraction_models.py
@@ -24,10 +24,10 @@ from .testing_common import PeftCommonTester, PeftTestConfigManager


 PEFT_FEATURE_EXTRACTION_MODELS_TO_TEST = [
-    "hf-internal-testing/tiny-random-BertModel",
-    "hf-internal-testing/tiny-random-RobertaModel",
-    "hf-internal-testing/tiny-random-DebertaModel",
-    "hf-internal-testing/tiny-random-DebertaV2Model",
+    "/opt/big_models/tiny-random-BertModel",
+    "/opt/big_models/tiny-random-RobertaModel",
+    "/opt/big_models/tiny-random-DebertaModel",
+    "/opt/big_models/tiny-random-DebertaV2Model",
 ]

 FULL_GRID = {
diff --git a/tests/test_hub_features.py b/tests/test_hub_features.py
deleted file mode 100644
index 44e9d6b..0000000
--- a/tests/test_hub_features.py
+++ /dev/null
@@ -1,38 +0,0 @@
-# coding=utf-8
-# Copyright 2023-present the HuggingFace Inc. team.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import unittest
-
-from transformers import AutoModelForCausalLM
-
-from peft import PeftConfig, PeftModel
-
-
-PEFT_MODELS_TO_TEST = [("peft-internal-testing/test-lora-subfolder", "test")]
-
-
-class PeftHubFeaturesTester(unittest.TestCase):
-    def test_subfolder(self):
-        r"""
-        Test if subfolder argument works as expected
-        """
-        for model_id, subfolder in PEFT_MODELS_TO_TEST:
-            config = PeftConfig.from_pretrained(model_id, subfolder=subfolder)
-
-            model = AutoModelForCausalLM.from_pretrained(
-                config.base_model_name_or_path,
-            )
-            model = PeftModel.from_pretrained(model, model_id, subfolder=subfolder)
-
-            self.assertTrue(isinstance(model, PeftModel))
diff --git a/tests/test_mixed.py b/tests/test_mixed.py
index ea35df3..4409751 100644
--- a/tests/test_mixed.py
+++ b/tests/test_mixed.py
@@ -722,7 +722,7 @@ class TestMixedAdapterTypes(unittest.TestCase):
         # test a somewhat realistic model instead of a toy model
         torch.manual_seed(0)

-        model_id = "hf-internal-testing/tiny-random-OPTForCausalLM"
+        model_id = "/opt/big_models/tiny-random-OPTForCausalLM"
         model = AutoModelForCausalLM.from_pretrained(model_id).eval().to(self.torch_device)
         input_ids = torch.tensor([[1, 1, 1], [1, 2, 1]]).to(self.torch_device)
         attention_mask = torch.tensor([[1, 1, 1], [1, 0, 1]]).to(self.torch_device)
diff --git a/tests/test_multitask_prompt_tuning.py b/tests/test_multitask_prompt_tuning.py
index be548aa..9ae98f1 100644
--- a/tests/test_multitask_prompt_tuning.py
+++ b/tests/test_multitask_prompt_tuning.py
@@ -241,7 +241,7 @@ class MultiTaskPromptTuningTester(TestCase, PeftCommonTester):
         task_ids = torch.tensor([1, 2]).to(self.torch_device)

         original = LlamaForCausalLM.from_pretrained(
-            "trl-internal-testing/tiny-random-LlamaForCausalLM", torch_dtype=torch.bfloat16
+            "/opt/big_models/tiny-random-LlamaForCausalLM", torch_dtype=torch.bfloat16
         )
         mpt = get_peft_model(original, self._create_multitask_prompt_tuning_config())
         mpt = mpt.to(self.torch_device)
diff --git a/tests/test_stablediffusion.py b/tests/test_stablediffusion.py
index b87c61c..819c217 100644
--- a/tests/test_stablediffusion.py
+++ b/tests/test_stablediffusion.py
@@ -26,7 +26,7 @@ from .testing_common import ClassInstantier, PeftCommonTester
 from .testing_utils import temp_seed


-PEFT_DIFFUSERS_SD_MODELS_TO_TEST = ["hf-internal-testing/tiny-stable-diffusion-torch"]
+PEFT_DIFFUSERS_SD_MODELS_TO_TEST = ["/opt/big_models/tiny-stable-diffusion-torch"]
 CONFIG_TESTING_KWARGS = (
     {
         "text_encoder": {
diff --git a/tests/test_tuners_utils.py b/tests/test_tuners_utils.py
index 7cc0a17..f7a7b79 100644
--- a/tests/test_tuners_utils.py
+++ b/tests/test_tuners_utils.py
@@ -107,7 +107,7 @@ class PeftCustomKwargsTester(unittest.TestCase):
     def test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):
         # We use a LoRA Config for testing, but the regex matching function is common for all BaseTuner subclasses.
         # example model_id for config initialization. key is matched only against the target_modules given, so this can be any model
-        model_id = "peft-internal-testing/tiny-OPTForCausalLM-lora"
+        model_id = "/opt/big_models/tiny-OPTForCausalLM-lora"
         config = LoraConfig(
             base_model_name_or_path=model_id,
             target_modules=target_modules,
@@ -122,7 +122,7 @@ class PeftCustomKwargsTester(unittest.TestCase):
         # users to easily debug their configuration. Here we only test a single case, not all possible combinations of
         # configs that could exist. This is okay as the method calls `check_target_module_exists` internally, which
         # has been extensively tested above.
-        model_id = "hf-internal-testing/tiny-random-BloomForCausalLM"
+        model_id = "/opt/big_models/tiny-random-BloomForCausalLM"
         model = self.transformers_class.from_pretrained(model_id)
         # by default, this model matches query_key_value
         config = LoraConfig()
@@ -145,7 +145,7 @@ class PeftCustomKwargsTester(unittest.TestCase):
             self.assertFalse(key in unmatched)

     def test_feedforward_matching_ia3(self):
-        model_id = "hf-internal-testing/tiny-random-T5ForConditionalGeneration"
+        model_id = "/opt/big_models/tiny-random-T5ForConditionalGeneration"
         model = self.transformers_class.from_pretrained(model_id)
         # simple example for just one t5 block for testing
         config_kwargs = {
